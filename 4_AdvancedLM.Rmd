---
title: "Advanced Linear Models"
output: html_document
---

# Overview

Here we will build on a base understanding of linear models in R using the `lm()` function. In the [Introduction to Linear Models Tutorial](https://colauttilab.github.io/RIntroStats/3_LinearModels.html) we reviewed the basic structure of linear models, which have a single continuous **response** or **dependent** variable and one or more **predictor** or **independent** variables. We walked through some specific examples, noting how they are all special cases of the linear model. 

# Setup

Load libraries and custom plotting theme

```{r, message=F}
library(ggplot2) # plotting library
library(dplyr) # data management

source("http://bit.ly/theme_pub") # Set custom plotting theme
theme_set(theme_pub())
```


## Predictor variables

```{r}
set.seed(234)
TestDat<-data.frame(
  PredCon=rnorm(1000),
  PredNom=sample(c("A","B"),1000,replace=T),
  Err=rnorm(1000)
)
```

Now we set up our response variable:

```{r}
Beta0<-10 # Intercept
Beta1<-1.3 # Slope of the continuous predictor
TestDat<-TestDat %>% 
  mutate(PredNomMean=recode(PredNom,"A"=0,"B"=1))# Means of the categorical predictor
```

# Interaction terms

Any model with two or more predictors has the potential for interaction terms. The interaction term means that we multiply the two values together and then by a new coefficient:

$$ Y_i \sim \beta_0 + \beta_1 X_{1,i} + \beta_2 X_{2,i} + \beta_3X_{1,i}X_{2,i}$$

So we can calculate our response variable and add it to the dataset. 

First, calculate the response without the interaction. Then, add a separate slope 
for group B.

```{r}
Beta3=1.2
TestDat<-TestDat %>%
  mutate(Resp=recode(PredNom,"A"=Beta0+Beta1*PredCon+PredNomMean+Err,
                     "B"=Beta0+Beta1*PredCon+PredNomMean+Beta3*PredCon+Err))
```

Using the `lm` function in R, we denote the interaction using a colon `:`.

```{r}
Mod1<-lm(Resp~PredCon+PredNom+PredCon:PredNom, data=TestDat)
anova(Mod1)
summary(Mod1)
```

Note that the interaction term gets its own test statistic. Looking at the **Estimate** column in the summary we can see that `PredNomA` is missing, meaning that it is the overall intercept, `PredCon` is the slope of the continuous variable for the A group ONLY. This is our mean of group A added to our overall intercept (0 + 10 = 9). To get the equation for the B group, we have to first add PredNomB to the intercept (10 + 1 = 11), which gives us our intercept for the B group. To get the slope of group B we add the interaction term to the slope for group A (1.2 + 1.3). We can use ggplot to visualize this:

## Visualizing interactions

```{r}
ggplot(aes(x=PredCon,y=Resp, group=PredNom),data=TestDat) +
  geom_point(aes(colour=PredNom)) + geom_smooth(method="lm")
```

The `geom_smooth()` function fit separate lines for each of the `group=` categories in the ggplot function. We can see the steeper slope an higher intercept for the B group (remember that the intercept is the value of Y when x=0).

## Two Categorical Predictors

The above example is for one categorical + one continuous, but what if we have two categorical variables?

```{r}
TestDat<-TestDat %>%
  mutate(PredNom2 = sample(c("Hot","Cold"),1000, replace=T)) %>%
  mutate(PredNom2Mean = recode(PredNom2,"Hot"=2,"Cold"=0)) %>%
  mutate(RespCat = recode(PredNom2,"Hot"=Beta0+PredNomMean+PredNom2Mean+Err,
                    "Cold"=Beta0+PredNomMean+PredNom2Mean+
                      Beta3*PredNomMean*PredNom2Mean+Err))
```

```{r}
Mod2<-lm(RespCat ~ PredNom + PredNom2 + PredNom:PredNom2, data=TestDat)
anova(Mod2)
summary(Mod2)
```

Looking at the summary, we can see that the intercept is calculated for Group A in the Cold treatment, with a mean of 9.4 for that group. All the other group means are calculated as deviations. 

From the coefficients, we see the intercept is 10, then add 0.8 for anything with group B, 2.0 for any hot treatment and 0.17 for only the B group in the hot treatment. Specifically:

  * The A group in the Cold treatment is 10
  * The B group in the Cold treatment is 10 + 0.8 = 10.8 
  * The A group in the Hot treatment is 10 + 2.0 = 12.0
  * The B group in the Hot treatment is 10 + 0.8 + 2.0 + 0.17 = 13.0

We can see the means with ggplot (or qplot):

```{r}
qplot(x=PredNom,fill=PredNom2,y=RespCat,geom="boxplot",data=TestDat)
```

## Two Continuous Predictors

Let's now consider two continuous predictors. This time it's a bit easier to set up the data:

```{r}
Beta2<- -2
TestDat<-TestDat %>%
  mutate(PredCon2=rnorm(1000)) %>%
  mutate(RespCon=Beta0 + Beta1*PredCon + Beta2*PredCon2 +
           Beta3*PredCon*PredCon2 + Err)
```

```{r}
Mod3<-lm(RespCon ~ PredCon + PredCon2 + PredCon:PredCon2, data=TestDat)
anova(Mod3)
summary(Mod3)
```

Again, we see in the **Estimate** column, but now the overall intercept is 10, because we only have 1 grouping variable. The slope for the PredCon variable is 1.3 and now we also have the slope for PredCon2 (NOTE: -2 is the slope, not the deviation in slope). Finally, we have the interaction term that we set at 1.2.

The interaction term for two continuous variables is more difficult to visualize than the two previous cases. This is a good example of how math can be so useful, as a single term can explain this fairly complex relationship.

Let's plot out our two continuous predictor variables and then scale/colour the points based on the predicted value from the model:

```{r}
ggplot(aes(x=PredCon,y=PredCon2),data=TestDat) +
  geom_point(aes(colour=predict(Mod3),size=predict(Mod3)),alpha=0.3)
```

You can see that the smallest points are in the top left and the largest are in the bottom right. This is what we expect given the slopes, but there is a bit of curvature that we can see with the larger points in the top right and bottom left.

Another way to think about this is by setting different values for one of the continuous variables to see how the slope changes. In this case, we will calculate the slope of the regression when our second predictor is at -2, 0 and +2:

```{r}
ggplot(aes(x=PredCon,y=RespCon,colour=PredCon2),data=TestDat) +
  geom_point() +
  geom_abline(intercept=10,slope=1.3+1.2*(0),colour="yellow") + # when PredCon2=0
  geom_abline(intercept=10,slope=1.3+1.2*(-2),colour="red") + # when PredCon2=-2
  geom_abline(intercept=10,slope=1.3+1.2*(2),colour="green") # when PredCon2=2
```

## `*` Shorthand

In larger linear models, we may have multiple predictors and it can become very long to write out all the individual terms and there intereactions. For example, if we have 3 predictors $X_1$, $X_2$ and $X_3$ we would have to write:

```{r, eval=F}
lm(Y ~ X1 + X2 + X3 +
     X1:X2 + X1:X3 + X2:X3 +
     X1:X2:X3)
```

We can use the shorthand `*` in `lm` to represent the interaction AND its individual terms. So, the above model could be written simply as:

```{r, eval=F}
lm(Y ~ X1*X2*X3)
```

From this example, we can also see how complicated our models can get as we add more predictors with more interaction terms. 

# Collinearity

Collinearity occurs when two or more variables have similar predictive value. An easy way to think about this is when you have one predictor variable that is a function of two or more variables. One way to test for collinearity, is to calculate the correlations among predictor variables. If you have high correlation then you should think if/how you can exclude one of the variables.

In addition, think whether some variables are just combinations of others. Here's an example:

```{r}
B<-rnorm(1000)
C<-rnorm(1000)
A<-B+C
cor(A,B)
cor(A,C)
```

C is a function of A and B, but there is enough variable in each that the correlation is not too strong. However, if we put all of these in a linear model, we can have problems:

```{r}
Y<-10+0.2*B+1.2*C+rnorm(100)
Mod6<-lm(Y ~ A + B + C)
summary(Mod6)
```

Notice the `NA` in our predictor, which is a **red flag** in the output of any linear model. IN this case, we get estimates for A and B, so adding C into the model adds no predictive power because C = A-B.

A similar problem can occur when two predictors are highly correlated, but not completely co-linear

```{r}
A<-B+C+rnorm(1000,sd=0.001)

Y<-10+0.2*B+1.2*C+rnorm(100)
Mod6<-lm(Y ~ A + B + C)
summary(Mod6)
```

In this case we don't get any NA values, but note how the P-values for A, B AND C are all non-significant. Even though they would be if we ran separate models:

```{r}
summary(lm(Y~A))
summary(lm(Y~B))
summary(lm(Y~C))
```

## Dealing with Collinearity  

The best way to deal with collinearity is to think carefully about the predictor variables you put into your model. If a predictor variable can itself be predicted by one or more other predictor variables, then it's best to exclude from the model. Deciding which variable to exclude depends on the specific biology of the system you are studying. There is no simple formula you can use, unfortunately.

Sometimes predictors are ALMOST collinear, but there is enough variation to test which predictor is a better predictor. We can do this with model selection.

Model selection also solves another problem: when we have many potential models, we can get significant models **just by chance**. For example, if we run 100 models on completely random data, we would expect 5 models, on average, to be significant at `p < 0.05`. 

Here's a more concrete example. A colleague of yours learns about linear models and decides to use them to find genes that promote or impede tumor growth. Using a large database they find a whopping 5,000 genes associated with tumor growth. They are particularly excited about a subset of 100 genes with `p < 0.01`

But you see the problem right away. Knowing that their dataset contains 100,000 SNPs (i.e. bp differences) from 10 million participants, you can calculate the **False Discovery Rate** (**FDR**). If you run 100,000 statistical tests, you expect 5% to have a `p < 0.05` and 1% to have `p < 0.01` **by chance alone!**

# FDR

A more general definition of the FDR applies to any model with a prediction and an observed (true) value. Think of a clinical test for Lyme disease as an example. The test is our model, from which we predict the status of the patient. The patient either has Lyme (positive) or not (negative) and the test is either accurate (true) or inaccurate (false). Thus, there are four possibilities:

  1. **True Positive (TP)** -- The test is positive AND the patient has the disease
  2. **True Negative (TN)** -- The test is positive AND the patient doesn't have the disease  
  3. **False positive (FP)** -- The test is positive BUT the patient doesn't have the disease
  4. **False negative (FN)** -- The test is negative BUT the patient has the disease

The false discovery rate is mathematically defined as

$$ FDR = \frac{FP}{FP+TP}$$

# Model Accuracy

In addition to FDR, we can calculate the accuracy of the model as the number of true predictions divided by the total. It is often expressed as a %

$$ Accuracy = \frac{TP + TN}{TP + TN + FP + FN} \times 100 \% $$

In addition to accuracy of the overall model, we can subdivide the performance of the model into sensitivity and specificity.

## Sensitivity

Sensitivity is the the number of true positives divided by the total positives. It answers the question "What percent of positive outcomes (e.g. uninfected individuals) are predicted by the model?"


$$ Sensitivity = \frac{TP}{TP+FN} \times 100 \%$$

## Specificity

Specificity is the inverse of sensitivity, in that it measures the number of accurate positive results. It answers the question "What percent of negative  outcomes (e.g. uninfected individuals) are predicted by the model?"

$$ Specificity = \frac{TN}{TN+FP} \times 100 \%$$

> If you had a choice between a test with high specificity OR high sensitivity, when would you choose one over the other?


# Model Selection

So far, we have framed linear models as significant or non-significant. But there are some problems with this approach, including the False Discovery problem and a few others outlined in this nice [YouTube video](https://youtu.be/42QuXLucH3Q) from Veritasium, and in this paper by John Ioannidis in [PLoS Medicine](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124.

An alternate approach to running mutliple independent models is to test multiple models to see which one is best supported by the data. Each model represents a different hypothesis, so **model selection is a way to test among alternative hypotheses**, rather than just testing a series of single hypotheses against the null.

## Goodness-of-Fit

In order to select among models, we need some metric that describes how well the data fit each model. These are known as **fit** or **goodness-of-fit**, and there are three main classes: $R^2$, Likelihood, and Information Criteria. 

## $R^2$

We have already seen this in our linear model outputs. The $R^2$ value ranges from 0 to 1 and describes how well the data fit the prediction. Let's look at two examples:

```{r}
X<-rnorm(100)
Y1<-rnorm(100)+0.3*X
Y2<-X
qplot(x=X,y=Y1)+geom_smooth(method="lm")
Rmod1<-lm(Y1~ X)
summary(Rmod1)
```


```{r}
qplot(x=X,y=Y2)+geom_smooth(method="lm")
Rmod2<-lm(Y2 ~ X)
summary(Rmod2)
```

More generally, the $R^2$ is the squared correlation between the predicted and the residuals of the model:

```{r}
cor(predict(Rmod1),Y1)^2
```
```{r}
cor(predict(Rmod2),Y2)^2
```

Notice that the first one is slightly different from the **Adjusted R-squared**, because the **Adjusted** corrects for random correlations that arise by chance. As sample size increases, the Adjusted R-squared approaches the actual R-squared (i.e. squared correlation).

Run this code a few times to see the range of $R^2$ values that come up:

```{r}
cor(rnorm(100),rnorm(100))^2
```

Since the Adjusted R-squared 

## ANOVA

If we have two tests that are **nested** then we can calculate an ANOVA on the residual sums of squares. Nested just means that one model is a subset of the other. For example, ModB is nested within ModA, and ModC is nested in both ModB and ModA in the following models:

```{r, eval=F}
ModA<-lm(Y ~ X1 + X2 + X1:X2)
ModB<-lm(Y ~ X1 + X2)
ModC<-lm(Y ~ X1)
```

In contrast, this model would not be nested because it contains a predictor that isn't found in the other models:

```{r, eval=F}
ModD<-lm(Y ~ X1 + X2 + X3)
```

We can test nested lm using the `anova` function in R, which tests the hypothesis that the larger or **full model** fits the data better than the smaller or **reduced model**. Here's a simple example:

```{r}
X1<-rnorm(1000)
X2<-rnorm(1000)
Y<-X + X2 + rnorm(1000)
Full<-lm(Y ~ X1 + X2)
Reduced<-lm(Y ~ X1)
```

Now, we can ask:

Does the Full model fit the data better than the reduced model? 

OR

Does adding X2 improve the model more than we would expect by chance?

```{r}
anova(Reduced,Full)
```

Note that this is an ANOVA with degrees of freedom equal to the difference in the number of parameters (1 regression coefficient in this case). 

## Likelihood & LRT

The likelhood ratio test gets its name from the statistic:

$$-2\ln(\frac{\theta_0}{\theta_1})$$

Where $\theta$ is the likelihood of a model, and the subscripts 0 and 1 are for two models that differ by at least one parameter. The model with **less parameters** goes in the numerator, and the model with **more parameters** goes in the **denominator**

The likelihood function is quite complicated, but for now just know that you can calculate the likelihood of any model. Taking the natural log of the ratio, and then multiplying by negative 2 gives you the **likelihood ratio**, which you can test against a $\chi^2$ distribution, with degrees of freedom equal to the difference in the degrees of freedom (number of predictors). 

Thus, the **LRT** is very similar to the `anova` function for linear models, except that the LRT uses the $\chi^2$ distribution of likelihoods whereas the `anova` uses the $F$-distribution of variances. 

## Information Criteria

Information criteria are another set of statistics that measure the fit of a model to the data. The key difference is that there is no statistical test, and no p-value. You just compare the information criteria of different models to find the best one. 

### AIC

The most common information criterion is the **Akike Information Criterion** or **AIC**. The mathematical definition of **AIC** is:

$$ AIC = 2k - \ln(\theta)$$

where $k$ is the number of parameters in the model, and $ln(\theta)$ is the natural log of the likelihood of the model. Yes, the same likelihood as the Likelihood Ratio Test (LRT)!

> The 'best' model is the one with the **lowest** AIC

Unlike the LRT, there is no p-value associated with AIC. However, a common rule of thumb for comparing models is to consider them to have a similar fit if the difference in AIC between the models is less than 2. This is the change in AIC, or delta AIC

$$\Delta _{AIC}$$

The **relative likelihood** of two models is:

$$ e^{1/2 (AIC_{small} - AIC_{big})} $$

where $AIC_{small}$ is the model with the **lower AIC** (i.e. better fit to the data), and $AIC_{big}$ is the model with the **higher AIC**. The value will be between zero (large difference) and 1 (small difference). For example, if we had AIC values of $AIC_1 = 25$ and $AIC_2 = 38$, our relative likelihood would be:

$$ e^{1/2(25-38)} = 0.001503439$$

We would say that the model for $AIC_2$ is 1.5% as probable as $AIC_1$. Meaning, that $AIC_1$ is a much better fit to the data. 

### AICc

Another common and important criterion is AICc. This is just the AIC adjusted for small sample size:

$$AICc = AIC +{\frac {2k^{2}+2k}{n-k-1}}  $$

Where $n$ is the sample size. As $n$ increases, the denominator of the second term moves toward zero. So as sample size increases, AICc approaches AIC. Note also that $k$ is in the denominator too, so we need larger sample sizes when including more complicated models.

### BIC

The **Bayes Information Criterion** or **BIC** is related to Bayes' theorem. It is similar to AICc but with a different adjustment for sample size:

$$ {BIC} =k\ln(n)-2\ln(\theta) $$
as above, $n$ denotes samples size, $k$ the number of parameters estimated in the model, and $\theta$ is the likelihood of the model.

# CAUTION: Missing Values

In addition to the usual assumptions of linear models, another important quality check for model selection is to **make sure that each model has the exact same observations**. For our simulated data it's no problem, but with real data you may be missing observations for certain predictor variables. In that case, you will have different data in models that include or exclude predictor variables with one or more missing values. 

A really simple way to deal with this is to make a dataset containing ONLY the predictor and response variables you want to use in model selection, and then use the `complete.cases` function to remove any rows with missing values.

Here's an example:

```{r}
Incomplete<-data.frame(A = c(0,1,2,3,4,5),
                       B = c(0,1,NA,3,4,5),
                       C = c(0,1,2,3,NA,5))

Incomplete
```

Our incomplete data.frame has 6 rows

Which rows have complete data?

```{r}
complete.cases(Incomplete)
```


Remove rows with missing data:
```{r}
Incomplete %>%
  filter(complete.cases(Incomplete))
```




