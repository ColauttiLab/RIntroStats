---
title: "Linear Models"
output: html_document
---

# Setup

Load libraries and our custom plotting theme.

```{r, message=F}
library(ggplot2) # plotting library
library(dplyr) # data management

source("http://bit.ly/theme_pub") # Set custom plotting theme
theme_set(theme_pub())
```

We will be working with the same `FallopiaData.csv` data that we used in the 
[R fundamentals tutorial](https://colauttilab.github.io/RCrashCourse/1_fundamentals.html) and the [`qplot` tutorial](https://colauttilab.github.io/RCrashCourse/2_qplot.html). We 
could download to a local folder again and then read from our computer.

Another trick is to read the file directly from the internet. This will work for 
any file available through `http://`

```{r}
InDat<-read.csv("http://colauttilab.github.io/RCrashCourse/FallopiaData.csv")
str(InDat)
```

The structure function `str()` is a handy way to inspect the data, but it 
doesn't show us all the different values for the nominal (i.e. categorical) columns. We can 
use the `unique()` function for that. We could apply it to each column 
or we can use the `dplyr()` function from the [data science](https://colauttilab.github.io/RCrashCourse/5_datascience.html) tutorial

```{r}
InDat %>%
  select(Scenario, Nutrients, Taxon) %>%
  unique()
```

This gives us every combination of the nominal variables. In this case, we 
can see that most of the different `Scenario` categories are only applied to the 
high `Nutrient` treatment. 

Looking at the combined output, we can see:

  * **PotNum** - a unique ID number for each plant pot
  * **Scenario** - the way fertilizer was applied
  * **Nutrients** - the amount of fertilizer applied
  * **Taxon** - the species of *Fallopia*
  * **Symphytum, Silene, Urtica, Geranium, Geum** - biomass of native plants, 
  in competition with one of the two *Fallopia* species grown
  * **Fallopia** - biomass of the *Fallopia* competitor species
  
# Overview

Most statistical textbooks and courses will go through all of the standard models 
including ANOVA and linear regression, then slightly more complicated models like 
polynomial regression, multiple regression, multifactor ANOVA, and ANCOVA.

I'm not sure why stats are always taught this way. There could be a good reason 
but I suspect it has more to do with the history of statistics: Student's t-test 
was invented, followed by Fisher's ANOVA, and then linear regression. Then these 
were expanded to more complicated cases.

However, today we know that all of these models are special cases of a general 
model in statistics called **Linear Models**. In R, linear models are run with 
the `lm()` function. Rather than go through the history of statistics, we'll 
focus on the application of linear models. That said, it's important to know 
the difference between all those models listed above because many people learn 
statistics that way, so they are used to talking about those models.

> NOTE: Linear Models are different from General Linear Models, which are both 
different from Generalized Linear Models. We also have Linear Mixed Effects Models. 
For now, it's just important to know that these are different classes of models 
even though they sound similar.

# Structure

In the [`qplot` tutorial](https://colauttilab.github.io/RCrashCourse/2_qplot.html) 
we looked at visualizations for individual and paired data. 

We saw that data can come in a few flavours:

  * **Binary** -- Boolean variables with two possible states, such as 
  1 or 0, true/false, yes/no
  * **Nominal** -- Variables with two or more categories (e.g. treatment)
  * **Continuous** -- Measurements that can take on a range of values (e.g. height)
  * **Ordinal** -- Discrete categories but ordered in some way (e.g. number of 
  offspring)

Different types of linear models (e.g. ANOVA vs Regression) are defined by the 
type of input data they use. Importantly, Linear Models have two types of input: 

  * **Response** or **Dependent** variables -- these are the data columns that 
  we are trying to predict.
  * **Predictor** or **Independent** variables -- thes are the data columns that 
  we are using to make predictions.
  
Linear Models will always have a single, continuous response variable, but it 
can have one or more predictor variables that may include a mix of binary, 
nominal and continuous variables. Ordinal variables are a bit more tricky, 
and will be treated as (approximately) nominal or continuous variables, 
depending on the nature of the data. 

## Formula

To understand the syntax of a linear model, look at the help:

```{r, eval=F}
?lm
```

There are just two main components; the rest we can keep as default for no:

  * **formula** -- this is how we define the model
  * **data** -- the data.frame object we are analyzing

The formula for linear models looks like this:

`Response ~ Predictor(s)`

You just have to remember the tilde and which side to put your predictor vs 
response variables.

# Visualizing Types

Let's start by looking at a single response variable and a single predictor. 
Our response will always be a continuous variable in a Linear Model. Similar to 
the qplot tutorial, we can organize our linear models based on the type of 
predictor

To visualize these models, let's first create some data for each type of 
response variables:

## Continuous

Our continuous response variable (Y)

```{r}
set.seed(123)
Y<-rnorm(1000)
```

A continuous response variable (Xcon)

```{r}
set.seed(234)
Xcon<-rnorm(1000)
```

## Nominal

A categorical response variable (Xcat). 

We can use the sample function `?sample`

```{r}
set.seed(345)
Xcat<-sample(c("A","B","C"),1000, replace=T)
```


## Ordinal

What about ordinal data? There are more advanced models that can handle this 
kind of data but we can also run them in a linear model if we make a choice 
whether to treat it as categorical or continuous. 

For example, if we were looking at offspring numbers in Canadian 
moose, most would have 0, 1 or 2, but some would have 3 or more. We could 
recode every observation into one of 4 categories: 0, 1, 2, 3+ and analyze 
offspring number as a categorical variable.

Another example, if we look at the nubmer of seeds on the head of a dandelion, we 
might find that the distribution follows a normal or log-normal distribution. In 
that case we can treat it as a continuous variable -- possibly taking the log 
before using it in our linear model.

# Linear Regression

Linear regression uses a continuous predictor

`Continuous Response ~ Continuous Predictor`

```{r}
CCplot<-qplot(x=Xcon, y=Y)
CCplot
```

In linear regression, we want to find a straight line that fits to the majority 
of data. We can visulaize this using `geom_smooth()`

```{r}
CCplot<-CCplot + geom_smooth(method="lm")
```


```{r}
lm(Y ~ Xcon)
```

The output of the `lm` function is not too informative. It just shows us the 
input formula and then the estimated coefficients. There are some additional 
functions to extract more information about the model. 

```{r}
LinReg<-lm(Y ~ Xcon)
summary(LinReg)
```

## Predictions

The **Estimate** column gives us the equation for the line of best fit, with the 
intercept and slope. Recall the linear equation:

$$ y = mX + b $$

In linear models, this exact same equation is usually written as:

$$ Y_i \sim \beta_0 + \beta_1 X_i$$

This gives us the equation for the estimated line, which would be okay if every 
single observation fell exactly on the line of best fit. We can plot for comparison:

```{r}
CCplot
CCplot + geom_abline(slope=0.033, intercept=0.017, colour="red")
```

The red line that we added sits right on top of the `lm` line added from `geom_smooth`.

We can also make specific predictions for each observation:

```{r}
Predicted<- 0.033*Xcon + 0.017
CCplot + geom_point(aes(y=Predicted), colour="red")
```


However, we can see that most observations do not fall on the line of best fit. 
To fully describe the relationship between Y and X, we need to account for the 
deviation of each point from the line.

## Residual Error

This is called the **error term** or **residual error**:

$$ \epsilon_i = Observed_i - Predicted_i $$

In other words, we:

  1. Look at each point
  2. Record the observed value on the y-axis
  3. Look at the predicted value on the y-axis (the line of best fit)
  4. Subtract the observed value from the predicted
  
```{r}
Resid<-Y-Predicted
head(Resid)
```
  
  
Accounting for this error, fully describes each point, giving the correct 
Linear Model equation: 

$$ Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$

Our regression model has three important vectors:

## Predictor 

Take a minute to review and make sure you understand this. The $Y_i$ is the 
observed value of our response variable. The subscript $i$ represents an individual 
value, which tells us that $Y$ is a vector of values.

## Response

Similarly, the $X_i$ is the observed value of our predictor variable, with the 
same subscript, meaning that $X$ is also a vector. 

## Residuals

And again, $\epsilon$ is also a vector. This vector represents the difference 
between the observed Y and the predicted Y from our model. This is known as 
the vector of **residuals** or **residual error**. We can get these from our `lm` object. 

```{r}
LinResid<-residuals(LinReg)
head(LinResid)
```

Compare with our calculation above:

```{r}
head(Resid)
```

## Significance

Looking again at our linear regression output, we can see that each estimate 
has its own **standard error** , **t-value** and **p-value**.

```{r}
LinReg<-lm(Y ~ Xcon)
summary(LinReg)
```

In our [Distributions Tutorial](https://colauttilab.github.io/RIntroStats/1_Distributions.html) we looked at the t-test equation for a mean and standard error. 
That same equation can be applied to the estimate and standard error for the 
slope and intercept, under the null hypothesis that `slope = 0` and `intercept = mean(Y)`.

We can also see the **Adjusted R-squared** value, which is very close to zero. 
This R-squared value is an estimate of the amount of variation explained by the 
statistical model, rangiong from 0 (no variation explained) to 1 (100% explained).

The **F-statistic** tests the overall fit of the model. In this case you can see 
that our model is not significant. We can see this more clearly using the `anova` 
function

```{r}
anova(LinReg)
```


Let's try to make a significant model by 
creating a predictor that is related to the response. A simple way is to add a 
random value to each observation:

```{r}
CorPred<-Y+rnorm(length(Y))
CorMod<-lm(Y ~ CorPred)
qplot(x=CorPred,y=Y) + geom_smooth(method="lm")
summary(CorMod)
anova(CorMod)
```

Now we get a significant slope but not a significant intercept, based on the 
t-tests of the estimates.

We also get a highly significant F-test and our model explains almost 50% of the
variation.

# ANOVA

Let's look at a linear model with a categorical predictor.

```{r}
CatMod<-lm(Y ~ Xcat)
qplot(x=Xcat, y=Y, geom="boxplot")
summary(CatMod)
```

Similar to linear regrssion we have an estimate column, and for each estimate we 
also have standard error, t-value and probability.

We also have R-squared and F-values describing the fit of the model, along with 
an overall p-value for the model. 

BUT there is one important difference. Our predictor variable $X_i$ is categorical 
in this model, rather than continuous. So how do we get from a categorical 
predictor to an estimated slope?

We have 3 categories: A, B, C and we also have 3 estimates: `(Intercept)`, `XcatB`, 
and `XcatC`

We can define 3 categories using 2 binary variables.

```{r}
RecodeDat<-data.frame(Response=Y,
                      PredGroup=Xcat) %>%
  mutate(XcatB=recode(Xcat,"A"=0,"B"=1,"C"=0),
         XcatC=recode(Xcat,"A"=0,"B"=0,"C"=1))
```

> Why no XcatA?

We know that `Xcat=A` when `XcatB=0` AND `XcatC=0`. 

> Any predictor with N categories can be recoded into N-1 binary columns

Check that the data are recoded properly

```{r}
head(RecodeDat)
```
And run the model

```{r}
RecLM<-lm(Response ~ XcatB + XcatC, data=RecodeDat)
summary(RecLM)
```

and compare to the original ANOVA

```{r}
summary(CatMod)
```

This is equivalent to taking the deviation of the means:

```{r}
A<-RecodeDat %>% 
  filter(PredGroup=="A") %>%
  summarize(mean(Response))
B<-RecodeDat %>% 
  filter(PredGroup=="B") %>%
  summarize(mean(Response))
C<-RecodeDat %>% 
  filter(PredGroup=="C") %>%
  summarize(mean(Response))

A
B-A
C-A
```


Let's now look at the output of the `anova` function

```{r}
anova(CatMod)
```

Notice how there is now just a single predictor (Xcat) rather than separate 
predictors for each binary subcategory.

The equation for this ANOVA can be written the same as the regression model:

$$ Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$

We just have to keep in mind that $X_i$ is a nominal variable with N categories 
recoded as N-1 binary variables. $\beta_0$ is the mean of one category of data (Intercept). 
There are N-1 $\beta_1$ terms specifying the deviation of each group from the Intercept.

> NOTE: The `lm` funciton chooses the category for $\beta_0$ as the first in alphabetical order

Contrast this with a continuous variable where $\beta_1$ is a single value for 
the slope, which is multiplied by $X_i$

# 2+ predictors

So far we have looked at models with a single predictor. What if we have 2 or 
more predictors?

We can simply add additional predictor $X$ terms to our linear model 

$$ Y_i = \beta_0 + \beta_1 X_{1,i} + \beta_2 X_{2,i} + ... + \epsilon_i$$

There are a few specific 'flavours' of these models, as shown below. You should 
know these because they are still used in the scientific literature, but remember 
that they are just different combinations of predictor variables:

  * **ANCOVA** -- **nominal** + **continuous** predictors
  * **Multifactor ANOVA** -- 2 or more **nominal** predictors
  * **Multiple regression** -- 2 or more **continuous** predictors
  * **Polynomical regression** -- 1 or more continuous predictors raised to 2 or 
  more powers

## Build-a-model

Now that you understand the basic structure of a linear model, let's build one 
using simulated data in R, with a few different parameters we can adjust to see 
how these changes affect the output of `lm`. 

In some of the above models, we made $Y$ *independent* of $X$ by randomly sampling 
$Y$ from a normal distribution. In the models below, we will make $Y$ *dependent* 
on the predictors.

In each case, we'll look at the linear model equation, and code it in R.

## ANCOVA

ANCOVA is short for ANalysis of COVariance, meaning it is is an ANOVA (nominal 
predictor $cat$) combined with a covariate (continuous predictor $con$).

$$ Y_i = \beta_0 + \beta_{cat} X_{cat,i} + \beta_{con} X_{con,i} + \epsilon_i$$

Let's imagine a cancer drug trial where we want to test the effect of a drug (+ placebo) 
and we want to include age as a covariate. Our response variable would have to be 
a continuous measurement -- let's say tumor growth.

First, let's create our predictor variables

```{r}
N<-1000 # Sample Size
Beta0<-0 # Overall mean
BetaCon<-0.03 # Slope of our continuous variable
SimDat<-data.frame(
  Treat=sample(c("Drug","Placebo"), N, replace=T), # Treatment
  Age=sample(18:80, N, replace=T), # Age (between 18 and 80 years)
  Err=rnorm(N, mean=0, sd=1) # Individual error with a mean of 0 and sd of 1
) %>%
  mutate(TreatMean=recode(Treat,"Drug"=-0.5,"Placebo"=1.2)) # Means of treatments
head(SimDat)
```

Now we just plug those into our linear model equation to generate our dependent 
variable Growth. Then plot the data and run `lm`:

```{r}
SimDat<-SimDat %>%
  mutate(Growth = Beta0 + TreatMean + BetaCon*Age + Err)

qplot(x=Age, y=Growth, colour=Treat, data=SimDat) + geom_smooth(method="lm")
GrowthMod<-lm(Growth ~ Age + Treat, data=SimDat)
anova(GrowthMod)
summary(GrowthMod)
```

> Try changing the parameters below re-running the code above. How 
does the graph and statstical output change?

  * `N` -- Sample Size
  * `Beta0` # Overall mean
  * `BetaCo`n # Slope of our continuous variable
  * `Err=rnorm(..., mean=??, sd=??)` # mean & sd of error term
  * `TreatMean=recode(Treat,"Drug"=??,"Placebo"=??))` # Means of treatments


## Multifactor ANOVA

A multifactor ANOVA is an analysis of variance with 2 or more categories. For 
example, if we wanted to test tumour growth from three chemicals on male vs female mice. 

$$ Y_i = \beta_0 + \beta_1 X_{1,i} + \beta_2 X_{2,i} + \epsilon_i$$

```{r}
N<-1000 # Sample Size
Beta0<-0 # Overall mean
SimDat<-data.frame(
  Treat=sample(c("Control","DrugA","DrugB","DrugC"), N, replace=T), # Treatment
  Sex=sample(c("M","F"), N, replace=T), # Sex
  Err=rnorm(N, mean=0, sd=1) # Individual error with a mean of 0 and sd of 1
) %>%
  mutate(TreatMean=recode(Treat,"Control"=0,"DrugA"=1.2,"DrugB"=-2,"DrugC"=0),
         SexMean=recode(Sex,"M"=0.5,"F"=-0.5)) # Means of treatments
head(SimDat)
```

```{r}
SimDat<-SimDat %>%
  mutate(Growth = Beta0 + TreatMean + SexMean + Err)

qplot(x=Treat, y=Growth, fill=Sex, data=SimDat, geom="boxplot")
GrowthMod<-lm(Growth ~ Sex + Treat, data=SimDat)
anova(GrowthMod)
summary(GrowthMod)
```

## Multiple regression

A multiple regression is just a regression with 2+ continuous predictors. 

> CHALLENGE: Make up an experiment with two predictors and try to encode them

$$ Y_i = \beta_0 + \beta_1 X_{1,i} + \beta_2 X_{2,i} + \epsilon_i$$

```{r}
N<-1000 # Sample Size
Beta0<-0 # Overall mean
SlopeA<-0.3 # Slope for 1st continuous predictor
SlopeB<- -0.4 # Slope for 2nd continuous predictor
SimDat<-data.frame(
  TreatA=sample(0:100,N, replace=T),
  TreatB=sample(0:100,N, replace=T),
  Err=rnorm(N, mean=0, sd=1) # Individual error with a mean of 0 and sd of 1
) %>%
  mutate(Growth = Beta0 + SlopeA*TreatA + SlopeB*TreatB + Err)
```

Visualizing the model is tricky because we have two predictors now. We could 
plot each separately on the x-axis, or we could scale point size to one of them

```{r}
qplot(x=TreatA, y=Growth, data=SimDat) + geom_smooth(method="lm")
qplot(x=TreatB, y=Growth, data=SimDat) + geom_smooth(method="lm")
GrowthMod<-lm(Growth ~ TreatA + TreatB, data=SimDat)
anova(GrowthMod)
summary(GrowthMod)
```

## Polynomial regression

A polynomial regression is a special type of multiple regression where the 
different predictors are polynomials of the same variable. 

$$ Y_i = \beta_0 + \beta_1 X_{1,i} + \beta_2 X^2_{1,i} + \beta_n X^n_{1,i} + ... + \epsilon_i$$
This is easier to understand with an example. 

Let's say we want to look at the relationship between plant 
height and plant biomass, with biomass following a quadratic relationship:

```{r}
N<-1000 # Sample Size
Beta0<-0 # Overall mean
SlopeA<-0.3 # Slope for 1st continuous predictor
SlopeB<- -0.4 # Slope for 2nd (quadratic) predictor
SimDat<-data.frame(
  Height=sample(c(0:1000)/100,N, replace=T),
  Err=rnorm(N, mean=0, sd=1) # Individual error with a mean of 0 and sd of 1
) %>%
  mutate(Biomass = Beta0 + SlopeA*Height + SlopeB*Height^2 + Err)

qplot(x=Height, y=Biomass, data=SimDat) + geom_smooth(method="lm")
```

To fit a polynomial, we use the Identity `I` function inside the lm

```{r}
GrowthMod<-lm(Biomass ~ Height + I(Height^2) , data=SimDat)
anova(GrowthMod)
summary(GrowthMod)
```

Let's look at a qubic relationship for comparison:

```{r}
SlopeC<- -2
SimDat<-SimDat %>%
  mutate(Biomass2 = Beta0 + SlopeA*Height + SlopeB*Height^2 + SlopeC*Height^3 + Err)
```

```{r}
GrowthMod<-lm(Biomass2 ~ Height + I(Height^2) + I(Height^3), data=SimDat)
anova(GrowthMod)
summary(GrowthMod)
```

Notice that our third polynomial is not significant, even though it should be 
because of our linear model. The reason for this is complicated, but it has to 
do with the fact that the polynomial predictors are not independent.

We can solve this problem with the `poly()` function, but note that it changes 
the estimates.


```{r}
GrowthMod<-lm(Biomass2 ~ poly(Height,degree=3), data=SimDat)
anova(GrowthMod)
summary(GrowthMod)
```
We can also add the polynomial term to `ggplot` object if we use the `formula` 
parameter in `geom_smooth`

```{r}
qplot(x=Height, y=Biomass2, data=SimDat) + geom_smooth(formula=y ~ poly(x,3, raw=TRUE))
```


# Quality Control

The key assumption of our linear models is:

**residual error follows a normal distribution with a mean of zero**

AND

**error variance is independent of predictors**

There are a few convenient ways to check this, but first, let's set up a few 
models, starting with a base dataset.


```{r}
N<-1000
B0<-0
B1<-0.03
TestDat<-data.frame(
  PredCon=sample(c(1:100)/10,N,replace=T),
  PredNom=sample(c("A","B","C"), N, replace=T),
  Err=rnorm(N, mean=0, sd=1)
) %>% 
  mutate(PredNomMean=recode(PredNom,"A"=-5,"B"=5,"C"=0)) %>%
  mutate(Response=B0+B1*PredCon+PredNomMean+Err)

TestMod<-lm(Response ~ PredCon + PredNom, data=TestDat)
anova(TestMod)
summary(TestMod)
```

This model should meet our assumptions, but let's check. There are three main 
ways to do this, all using the residuals of our model, which we can calculate 
manually, or use the `residuals` function on our model object.

## Histogram

```{r}
qplot(residuals(TestMod))
```

This looks normal, and is senered around zero, but what is a normal distribution 
supposed to look like? We can use our `qnorm` function to generate the expected 
quantile of each observation. We could then plot the observed vs. expected values.

This is called the quantile-quantile or q-q plot

## Q-Q Plot

The Quantile-Quantile (Q-Q) Plot bins the residuals the same way we would for a 
histogram, but then calculates the number of observations we expect in each bin 
for a normal distribution. If the data are perfectly normal, then all the points 
will fall along the 1:1 line

```{r}
qplot(sample=residuals(TestMod)) + stat_qq() + stat_qq_line()
```

We can see some very slight deviation from the line at the extreme values, where 
we have a small sample size

## Homoskedasticity

The last test is to make sure that variance is constant across our predictors. 
We can do this for each predictor variable alone, as well as the overall prediction 
from the model. To do this, let's add residuals to the dataset:

```{r}
TestDat$Residuals<-residuals(TestMod)
qplot(x=PredCon, y=Residuals, data=TestDat)
qplot(x=PredNom, y=Residuals, data=TestDat, geom="boxplot")
```

Note how the variance is similar range, centred at zero across all values of x 
in both graphs. We should also not see any correlation between the x- and y- axes.

We can also generate the predicted values using the estimates from the linear 
model output. But an easier way is the `predict()` function.

```{r}
TestDat$Predicted<-predict(TestMod)
qplot(x=Predicted, y=Residuals, data=TestDat)
```

Since we have continuous + nominal predictors we want to make sure that each 
group is centered at zero with similar variance with no correlations between 
the x- and y-axis.


# Violations

Let's make a couple of models that violate the assumptions for comparison.

## Nonlinear relationship

The first example is the one above -- fitting a linear model to a quadratic relationship

```{r}
# Make a new response variable
TestDat<-TestDat %>% 
  mutate(Response2=B0+B1*PredCon+0.2*PredCon^2+PredNomMean+Err)

# Run the linear model
TestMod2<-lm(Response2 ~ PredCon + PredNom, data=TestDat)

# Calculate predictions and residuals of the linear model
TestDat$Predicted2<-predict(TestMod2)
TestDat$Residuals2<-residuals(TestMod2)

# Plot to look for problems
qplot(x=Residuals2, data=TestDat) # Histogram 
qplot(sample=Residuals2, data=TestDat) + stat_qq() + stat_qq_line() # QQPlot
qplot(x=PredCon, y=Residuals2, data=TestDat)
qplot(x=PredNom, y=Residuals2, data=TestDat, geom="boxplot")
qplot(x=Predicted2, y=Residuals2, data=TestDat)
```

## Log-normal response

```{r}
# Make a new response variable
TestDat<-TestDat %>% 
  mutate(Response3=exp(B0+B1*PredCon+PredNomMean+Err))

# Run the linear model
TestMod3<-lm(Response3 ~ PredCon + PredNom, data=TestDat)

# Calculate predictions and residuals of the linear model
TestDat$Predicted3<-predict(TestMod3)
TestDat$Residuals3<-residuals(TestMod3)

# Plot to look for problems
qplot(x=Residuals3, data=TestDat) # Histogram 
qplot(sample=Residuals3, data=TestDat) + stat_qq() + stat_qq_line() # QQPlot
qplot(x=PredCon, y=Residuals3, data=TestDat)
qplot(x=PredNom, y=Residuals3, data=TestDat, geom="boxplot")
qplot(x=Predicted, y=Residuals3, data=TestDat)
```

## Error scales with prediction

```{r}
# Make a new response variable
TestDat<-TestDat %>% 
  mutate(Response4=B0+B1*PredCon*Err+PredNomMean*Err/10)

# Run the linear model
TestMod4<-lm(Response4 ~ PredCon + PredNom, data=TestDat)

# Calculate predictions and residuals of the linear model
TestDat$Predicted4<-predict(TestMod4)
TestDat$Residuals4<-residuals(TestMod4)

# Plot to look for problems
qplot(x=Residuals4, data=TestDat) # Histogram 
qplot(sample=Residuals4, data=TestDat) + stat_qq() + stat_qq_line() # QQPlot
qplot(x=PredCon, y=Residuals4, data=TestDat)
qplot(x=PredNom, y=Residuals4, data=TestDat, geom="boxplot")
qplot(x=Predicted, y=Residuals4, data=TestDat)
```

## Poisson Error

```{r}
# Make a new response variable
TestDat<-TestDat %>% 
  mutate(Response5=B0+B1*PredCon+PredNomMean+rpois(N,lambda=1))

# Run the linear model
TestMod5<-lm(Response5 ~ PredCon + PredNom, data=TestDat)

# Calculate predictions and residuals of the linear model
TestDat$Predicted5<-predict(TestMod5)
TestDat$Residuals5<-residuals(TestMod5)

# Plot to look for problems
qplot(x=Residuals5, data=TestDat) # Histogram 
qplot(sample=Residuals5, data=TestDat) + stat_qq() + stat_qq_line() # QQPlot
qplot(x=PredCon, y=Residuals5, data=TestDat)
qplot(x=PredNom, y=Residuals5, data=TestDat, geom="boxplot")
qplot(x=Predicted, y=Residuals5, data=TestDat)
```

## Residuals matter most

Note that in our analysis above, we are analyzing the RESIDUALS. Our assumptions 
with linear models are about the residuals NOT the predictor or response variables.

Let's look back at our original, well-behaved model as an example. Compare the 
histograms of the residuals with the response and predictors:

```{r}
qplot(residuals(TestMod))
qplot(TestDat$Response) 
qplot(TestDat$PredCon) 
qplot(TestDat$PredNom) 
```

There is nothing wrong with the sample! It doesn't matter that the predictors and 
response are not normally distributed, as long as the residuals are.

